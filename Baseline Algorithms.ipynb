{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Algorithms for Forest Cover Type Predictions\n",
    "\n",
    "\n",
    "Data source: https://www.kaggle.com/c/forest-cover-type-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.svm import SVC\n",
    "%matplotlib inline\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest = pd.read_csv(\"data/train.csv\", index_col=0)\n",
    "original_fields = forest.columns.tolist()\n",
    "forest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Functions for Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def labelSoilType(row):\n",
    "    \"\"\"\n",
    "    Label soil types\n",
    "    \"\"\"\n",
    "    for i in range(len(row)):\n",
    "        if row[i] == 1:\n",
    "            return 'Soil_Type'+str(i)\n",
    "        \n",
    "def azimuth_to_abs(x):\n",
    "    \"\"\"\n",
    "    Only care about the absolute angle from 0 w/o respect to direction\n",
    "    \"\"\"\n",
    "    if x>180:\n",
    "        return 360-x\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "def removeColumns(df):\n",
    "    to_remove = [] # features to drop\n",
    "    for c in df.columns.tolist():\n",
    "        if df[c].std() == 0:\n",
    "            to_remove.append(c)\n",
    "    df = df.drop(to_remove, 1)\n",
    "    print(\"Dropped the following columns: \\n\")\n",
    "    for r in to_remove:\n",
    "        print (r)\n",
    "    return df, to_remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create Soil Type Buckets\n",
    "soil_types= ['Soil_Type1', 'Soil_Type2', 'Soil_Type3',\n",
    "       'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7',\n",
    "       'Soil_Type8', 'Soil_Type9', 'Soil_Type10', 'Soil_Type11',\n",
    "       'Soil_Type12', 'Soil_Type13', 'Soil_Type14', 'Soil_Type15',\n",
    "       'Soil_Type16', 'Soil_Type17', 'Soil_Type18', 'Soil_Type19',\n",
    "       'Soil_Type20', 'Soil_Type21', 'Soil_Type22', 'Soil_Type23',\n",
    "       'Soil_Type24', 'Soil_Type25', 'Soil_Type26', 'Soil_Type27',\n",
    "       'Soil_Type28', 'Soil_Type29', 'Soil_Type30', 'Soil_Type31',\n",
    "       'Soil_Type32', 'Soil_Type33', 'Soil_Type34', 'Soil_Type35',\n",
    "       'Soil_Type36', 'Soil_Type37', 'Soil_Type38', 'Soil_Type39',\n",
    "       'Soil_Type40']\n",
    "soil_data = pd.read_csv('soil_types.csv').set_index('Soil Type')\n",
    "forest['Soil Type'] = forest[soil_types+['Cover_Type']].apply(lambda row: labelSoilType(row), axis=1)\n",
    "\n",
    "#Merge soil data\n",
    "forest = pd.merge(forest, soil_data, how='left', left_on='Soil Type', right_index=True)\n",
    "del forest['Soil Type'] # Delete string column\n",
    "\n",
    "#Remove unnecessary columns and update soil list\n",
    "forest, removed = removeColumns(forest)\n",
    "for i in removed:    \n",
    "    for j in soil_types:\n",
    "        if i == j:\n",
    "            soil_types.pop(soil_types.index(j))\n",
    "    for k in original_fields:\n",
    "        if i == k:\n",
    "            original_fields.pop(original_fields.index(k))\n",
    "\n",
    "# Create feature to that transforms azimuth to its absolute value\n",
    "forest['Aspect2'] = forest.Aspect.map(azimuth_to_abs)\n",
    "forest['Aspect2'].astype(int)\n",
    "\n",
    "# Create feature that determines if the patch is above sea level\n",
    "forest['Above_Sealevel'] = (forest.Vertical_Distance_To_Hydrology>0).astype(int)\n",
    "\n",
    "# Bin the Elevation Feature: check the feature exploration notebook for motivation\n",
    "bins = [0, 2600, 3100, 8000]\n",
    "group_names = [1, 2, 3]\n",
    "forest['Elevation_Bucket'] = pd.cut(forest['Elevation'], bins, labels=group_names)\n",
    "forest['Elevation_0_2600'] = np.where(forest['Elevation_Bucket']== 1, 1, 0)\n",
    "forest['Elevation_2600_3100'] = np.where(forest['Elevation_Bucket']== 2, 1, 0)\n",
    "forest['Elevation_3100_8000'] = np.where(forest['Elevation_Bucket']== 3, 1, 0)\n",
    "forest['Elevation_0_2600'].astype(int)\n",
    "forest['Elevation_2600_3100'].astype(int)\n",
    "forest['Elevation_3100_8000'].astype(int)\n",
    "del forest['Elevation_Bucket']\n",
    "\n",
    "# Create a feature for no hillshade at 3pm\n",
    "forest['3PM_0_Hillshade'] = (forest.Hillshade_3pm == 0).astype(int)\n",
    "\n",
    "#Direct distance to hydrology\n",
    "forest['Direct_Distance_To_Hydrology'] = np.sqrt((forest.Vertical_Distance_To_Hydrology**2) + \\\n",
    "    (forest.Horizontal_Distance_To_Hydrology**2)).astype(float).round(2)\n",
    "\n",
    "column_list = forest.columns.tolist()\n",
    "column_list = [c for c in column_list if c[:9] != 'Soil_Type']\n",
    "column_list.insert(10, 'Direct_Distance_To_Hydrology')\n",
    "column_list.insert(11, 'Elevation_0_2600')\n",
    "column_list.insert(12, 'Elevation_2600_3100')\n",
    "column_list.insert(13, 'Elevation_3100_8000')\n",
    "column_list.insert(14, 'Aspect2')\n",
    "column_list.insert(15, 'Above_Sealevel')\n",
    "column_list.insert(16, '3PM_0_Hillshade')\n",
    "column_list.extend(soil_types)\n",
    "column_list.extend(['Cover_Type'])\n",
    "columns = []\n",
    "for col in column_list:\n",
    "    if col not in columns:\n",
    "        if col != 'Cover_Type':\n",
    "            columns.append(col)\n",
    "columns.append('Cover_Type')\n",
    "        \n",
    "forest = forest[columns]\n",
    "forest.fillna(0,inplace=True) # Replace nans with 0 for our soil type bins\n",
    "forest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Feature Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(forest.shape[1]-1):\n",
    "    for j in range(54):\n",
    "        if i != j:\n",
    "            forest[forest.columns.tolist()[i]+\"_\"+forest.columns.tolist()[j]] = forest[forest.columns.tolist()[i]]*forest[forest.columns.tolist()[j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Columns That Have No Value (again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "forest, removed = removeColumns(forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the continuous features\n",
    "###### We will try Normalization, Standardized Scaling, and MinMax Scaling\n",
    "###### Note: there is no need to impute any data points as this is a pretty clean data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transformData(df, to_remove):    \n",
    "    # Initialize lists and variables\n",
    "    chunk_size = 0.1 #Validation chunk size\n",
    "    seed = 0 # Use the same random seed to ensure consistent validation chunk usage\n",
    "\n",
    "    ranks = [] #array of importance rank of all features\n",
    "    X_all = [] # all features\n",
    "    X_all_add = [] # Additionally we will make a list of subsets\n",
    "    rem = [] # columns to be dropped\n",
    "    i_rem = [] # indexes of columns to be dropped\n",
    "    trans_list = [] # Transformations\n",
    "    comb = [] # combinations\n",
    "    comb.append(\"All+1.0\")\n",
    "\n",
    "    ratio_list = [0.75,0.50,0.25] #Select top 75%, 50%, 25% of features\n",
    "    features = [] # feature selection models\n",
    "    model_features = [] # names of feature selection models\n",
    "\n",
    "    # reorder the data to have continuous variables come first\n",
    "    continuous = [] # continuous variables\n",
    "    categorical = [] # categorical variables\n",
    "    final_columns = [] # final columns list\n",
    "    for col in df.columns.tolist():\n",
    "        if col in to_remove:\n",
    "            pass\n",
    "        elif col == 'Cover_Type':\n",
    "            pass\n",
    "        elif df[col].nunique() > 4:\n",
    "            continuous.append(col)\n",
    "        else:\n",
    "            categorical.append(col)\n",
    "\n",
    "    final_columns.extend(continuous)\n",
    "    final_columns.extend(categorical)\n",
    "    final_columns.append('Cover_Type')\n",
    "    df = df[final_columns]\n",
    "    num_rows, num_cols = df.shape\n",
    "    cols = df.columns\n",
    "    size = len(continuous) # Number of continuous columns\n",
    "\n",
    "    i_cols = []\n",
    "    for i in range(0,num_cols-1):\n",
    "        i_cols.append(i)\n",
    "\n",
    "    # Create the data arrays for model building\n",
    "    val_array = df.values\n",
    "    X = val_array[:,0:(num_cols-1)]\n",
    "    y = val_array[:,(num_cols-1)]\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=chunk_size, random_state=seed)\n",
    "    X_all.append(['Orig','All', X_train,X_val,1.0,cols[:num_cols-1],rem,ranks,i_cols,i_rem])\n",
    "\n",
    "    # Standardize the data\n",
    "\n",
    "    X_temp = StandardScaler().fit_transform(X_train[:,0:size])\n",
    "    X_val_temp = StandardScaler().fit_transform(X_val[:,0:size])\n",
    "\n",
    "    # Recombine data\n",
    "    X_con = np.concatenate((X_temp,X_train[:,size:]),axis=1)\n",
    "    X_val_con = np.concatenate((X_val_temp,X_val[:,size:]),axis=1)\n",
    "\n",
    "    X_all.append(['StdSca','All', X_con,X_val_con,1.0,cols,rem,ranks,i_cols,i_rem])\n",
    "\n",
    "    # MinMax Scale the data\n",
    "\n",
    "    X_temp = MinMaxScaler().fit_transform(X_train[:,0:size])\n",
    "    X_val_temp = MinMaxScaler().fit_transform(X_val[:,0:size])\n",
    "\n",
    "    # Recombine data\n",
    "    X_con = np.concatenate((X_temp,X_train[:,size:]),axis=1)\n",
    "    X_val_con = np.concatenate((X_val_temp,X_val[:,size:]),axis=1)\n",
    "\n",
    "    X_all.append(['MinMax', 'All', X_con,X_val_con,1.0,cols,rem,ranks,i_cols,i_rem])\n",
    "\n",
    "    #Normalize the data\n",
    "\n",
    "    X_temp = Normalizer().fit_transform(X_train[:,0:size])\n",
    "    X_val_temp = Normalizer().fit_transform(X_val[:,0:size])\n",
    "\n",
    "    # Recombine data\n",
    "    X_con = np.concatenate((X_temp,X_train[:,size:]),axis=1)\n",
    "    X_val_con = np.concatenate((X_val_temp,X_val[:,size:]),axis=1)\n",
    "\n",
    "    X_all.append(['Norm', 'All', X_con,X_val_con,1.0,cols,rem,ranks,i_cols,i_rem])\n",
    "\n",
    "    # Add transformation to the list\n",
    "    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n",
    "        trans_list.append(trans)\n",
    "    \n",
    "    return X_all, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create separate datasets for original and engineered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_original_features, y_train = transformData(forest[original_fields], removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_all_features, y_train = transformData(forest, removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Logistic Regression Grid Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_list = [\n",
    "    {\n",
    "        'model':SVC(),\n",
    "        'params':{ \n",
    "            'C':(1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3),\n",
    "            'kernel':('linear', 'poly', 'rbf', 'sigmoid', 'precomputed'),\n",
    "            'degree':(1, 2, 3, 4),\n",
    "            'gamma':('auto', 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3),\n",
    "            'coef0':(1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3),\n",
    "            'probability':(True, False),\n",
    "            'shrinking':(True, False),\n",
    "            'tol':(1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1),\n",
    "            'decision_function_shape':('ovo', 'ovr', None)\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'model':LogisticRegression(),\n",
    "        'params':{ \n",
    "            'penalty':('l1', 'l2'), \n",
    "            'dual':(True, False), \n",
    "            'C':(1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3), \n",
    "            'fit_intercept':(True, False), \n",
    "            'intercept_scaling':(1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3),\n",
    "            'max_iter':(1, 10, 100, 1000, 10000),\n",
    "            'solver':('newton-cg', 'lbfgs', 'liblinear', 'sag'),\n",
    "            'tol':(1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1),\n",
    "            'multi_class':('ovr', 'multinomial')\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Run Grid Search over the different data transformations    \n",
    "def gridSearch(dataset, model, params):\n",
    "    for trans, s, X, X_val, d, cols, rem, ra, i_cols, i_rem in dataset:\n",
    "        g = GridSearchCV(model, params, error_score=-999, verbose=3)\n",
    "        g.fit(X, y_train)\n",
    "        return trans, g.best_estimator_, g.best_score_, g.best_params_, g.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    {\n",
    "        'data':X_original_features, \n",
    "        'description':'original'\n",
    "    },\n",
    "    {\n",
    "        'data':X_all_features, \n",
    "        'description':'all_features'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for m in model_list:\n",
    "    for d in datasets:\n",
    "        print (m['model'])\n",
    "        print (m['params'])\n",
    "        d['trans'], d['best_estimator'], d['best_score'], d['best_params'], d['cv_results'] = gridSearch(d['data'], m['model'], m['params'])\n",
    "        print (d['trans'], d['best_estimator'], d['best_score'], d['best_params'], d['cv_results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
