{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Baseline Model Testing\n",
    "\n",
    "Data source: https://www.kaggle.com/c/forest-cover-type-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annakhazan/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import cross_validation\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import f_classif\n",
    "from IPython.core.display import display, HTML\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from feature_eng_function import feature_eng_forest, forest_interactions\n",
    "from confusion_matrix_score_function import confusion_matrix_scoring \n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest = pd.read_csv(\"data/train.csv\") \n",
    "forest = forest.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "original_cols = list(forest.columns)\n",
    "original_cols.remove('Cover_Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15121</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15122</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15123</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15124</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15125</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15126</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15127</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15128</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15129</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15130</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15131</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15132</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15133</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15134</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15135</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15136</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15137</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15138</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15139</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15140</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15141</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15142</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15143</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15144</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15145</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15146</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15147</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15148</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15149</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15150</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580983</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580984</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580985</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580986</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580987</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580988</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580989</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580990</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580991</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580992</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580993</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580994</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580995</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580996</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580997</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580998</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580999</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581001</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581002</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581003</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581004</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581005</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581006</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581007</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581008</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581009</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581010</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581011</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581012</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>565892 rows Ã— 0 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [15121, 15122, 15123, 15124, 15125, 15126, 15127, 15128, 15129, 15130, 15131, 15132, 15133, 15134, 15135, 15136, 15137, 15138, 15139, 15140, 15141, 15142, 15143, 15144, 15145, 15146, 15147, 15148, 15149, 15150, 15151, 15152, 15153, 15154, 15155, 15156, 15157, 15158, 15159, 15160, 15161, 15162, 15163, 15164, 15165, 15166, 15167, 15168, 15169, 15170, 15171, 15172, 15173, 15174, 15175, 15176, 15177, 15178, 15179, 15180, 15181, 15182, 15183, 15184, 15185, 15186, 15187, 15188, 15189, 15190, 15191, 15192, 15193, 15194, 15195, 15196, 15197, 15198, 15199, 15200, 15201, 15202, 15203, 15204, 15205, 15206, 15207, 15208, 15209, 15210, 15211, 15212, 15213, 15214, 15215, 15216, 15217, 15218, 15219, 15220, ...]\n",
       "\n",
       "[565892 rows x 0 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pd.read_csv('data/test.csv', header=0)[['Id']].set_index('Id')\n",
    "output.head()\n",
    "\n",
    "# y_test_kaggle_GBM = optimized_kaggle_GBM.predict(test_X)\n",
    "# results_kaggle_GBM = pd.DataFrame(test.Id)\n",
    "# results_kaggle_GBM['Cover_Type'] = y_test_kaggle_GBM\n",
    "# results_kaggle_GBM['Cover_Type'] = results_kaggle_GBM['Cover_Type'].astype(int)\n",
    "# results_kaggle_GBM.to_csv('submissions/results_kaggle_GBM.csv', index=False)\n",
    "# #make sure you manually delete the last line\n",
    "# # SCORE = 0.73838\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped the following columns: \n",
      "\n",
      "Wetmore\n",
      "Pachic Argiborolis\n",
      "Aquolis\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['Cover_Type'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-a41c9223ad19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mforest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_eng_forest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/train.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'soil_types.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mforest_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_eng_forest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/test.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'soil_types.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mforest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mforest_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforest_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moriginal_cols_with_soil_eng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/annakhazan/Desktop/LearningMachine/feature_eng_function.py\u001b[0m in \u001b[0;36mfeature_eng_forest\u001b[0;34m(data_file_path, soil_file_path)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Replace nans with 0 for our soil type bins\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/annakhazan/anaconda/envs/py36/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2051\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2052\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2053\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2054\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2055\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/annakhazan/anaconda/envs/py36/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2095\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2096\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2097\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2098\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2099\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/annakhazan/anaconda/envs/py36/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1228\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1230\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s not in index'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobjarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Cover_Type'] not in index\""
     ]
    }
   ],
   "source": [
    "forest = feature_eng_forest('data/train.csv', 'soil_types.csv')\n",
    "forest_test = feature_eng_forest('data/test.csv', 'soil_types.csv')\n",
    "forest = forest.iloc[:,1:]\n",
    "original_cols_with_soil_eng = list(forest.columns)\n",
    "original_cols_with_soil_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest = forest_interactions(forest)\n",
    "forest_test = forest_interactions(forest_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the continuous features\n",
    "###### We will try Normalization, Standardized Scaling, and MinMax Scaling\n",
    "###### Note: there is no need to impute any data points as this is a pretty clean data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chunk_size = 0.1 #Validation chunk size\n",
    "seed = 0 # Use the same random seed to ensure consistent validation chunk usage\n",
    "\n",
    "X_all = [] # all features\n",
    "X_all_add = [] # Additionally we will make a list of subsets\n",
    "trans_list = [] # Transformations\n",
    "comb = [] # combinations\n",
    "comb.append(\"All+1.0\")\n",
    "\n",
    "features = [] # feature selection models\n",
    "model_features = [] # names of feature selection models\n",
    "\n",
    "#Reorder the data to have continuous variables come first\n",
    "continuous = []\n",
    "categorical = []\n",
    "final_columns = []\n",
    "for col in forest.columns.tolist():\n",
    "    if col == 'Cover_Type':\n",
    "        pass\n",
    "    elif forest[col].nunique() > 4:\n",
    "        continuous.append(col)\n",
    "    else:\n",
    "        categorical.append(col)\n",
    "final_columns.extend(continuous)\n",
    "final_columns.extend(categorical)\n",
    "final_columns.append('Cover_Type')\n",
    "forest = forest[final_columns]\n",
    "num_row, num_cols = forest.shape\n",
    "cols = forest.columns\n",
    "size = len(continuous) # Number of continuous columns\n",
    "\n",
    "#Create the data arrays for model building\n",
    "val_array = forest.values\n",
    "X = val_array[:,0:(num_cols-1)]\n",
    "y = val_array[:,(num_cols-1)]\n",
    "X_train, X_val, y_train, y_val = cross_validation.train_test_split(X, y, test_size=chunk_size, random_state=seed)\n",
    "X_all.append(['Orig','All', X_train,X_val,cols[:num_cols-1]])\n",
    "\n",
    "# MinMax Scale the data\n",
    "\n",
    "X_temp = MinMaxScaler().fit_transform(X_train[:,0:size])\n",
    "X_val_temp = MinMaxScaler().fit_transform(X_val[:,0:size])\n",
    "\n",
    "# Recombine data\n",
    "X_con = np.concatenate((X_temp,X_train[:,size:]),axis=1)\n",
    "X_val_con = np.concatenate((X_val_temp,X_val[:,size:]),axis=1)\n",
    "\n",
    "X_all.append(['MinMax', 'All', X_con,X_val_con,cols])\n",
    "\n",
    "# Add transformation to the list\n",
    "for trans,name,X,X_val,cols_list in X_all:\n",
    "    trans_list.append(trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create classifiers and Grid Search\n",
    "- Logistic Regression\n",
    "- SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add Logistic Regression\n",
    "n = 'Logistic Regression'\n",
    "model_features.append(n)\n",
    "comb.append(n)\n",
    "features.append([n, LogisticRegression(random_state=seed),\n",
    "    {\n",
    "        'penalty':('l1', 'l2'),\n",
    "        'dual':(True, False),\n",
    "        'C':(1e-3, 1e-2,1e-1,1e0,1e1,1e2,1e3),\n",
    "        'fit_intercept':(True, False),\n",
    "        'intercept_scaling':(1e-3,1e-2,1e-1,1e0,1e1,1e2,1e3),\n",
    "        'max_iter':('newton-cg', 'lbfgs', 'liblinear', 'sag'),\n",
    "        'tol':(1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1),\n",
    "        'multi_class':('ovr', 'multinomial')\n",
    "    }])\n",
    "    \n",
    "# Add SVM\n",
    "n = 'SVM'\n",
    "model_features.append(n)\n",
    "comb.append(n)\n",
    "features.append([n, LinearSVC(random_state=seed),\n",
    "    {\n",
    "        'C':(1e-3,1e-2,1e-1,1e0,1e1,1e2,1e3),\n",
    "        'kernel':('linear', 'poly', 'rbf', 'sigmoid', 'precomputed'),\n",
    "        'degree':(1,2,3,4),\n",
    "        'gamma':('auto',1e-3, 1e-2,1e-1,1e0,1e1,1e2,1e3),\n",
    "        'coef0':(1e-3, 1e-2,1e-1,1e0,1e1,1e2,1e3),\n",
    "        'probability':(True,False),\n",
    "        'shrinking':(True,False),\n",
    "        'tol':(1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1),\n",
    "        'decision_function_shape':('ovo', 'ovr', None)\n",
    "    }])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 100 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top100 = list(pd.read_csv('top_100.csv', header=None, names=['Feature', 'Importance'])['Feature'].values)\n",
    "column_lists = {\n",
    "    'original':original_cols,\n",
    "    'original with soil engineered':original_cols_with_soil_eng,\n",
    "    'top 100':top100#,\n",
    "    #'all':all_interacted_cols\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run grid search over the different data transformations\n",
    "def gridSearch(model, params, X, y):\n",
    "    g = GridSearchCV(model, params, error_score=-999, verbose=1)\n",
    "    g.fit(X, y)\n",
    "    return g.best_estimator_, g.best_score_, g.best_params_, g.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run models on selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "original\n",
      "54\n",
      "0.630952380952\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 55 features per sample; expecting 51",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-49c5acb686ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0;31m#Output prediction on test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s_%s_%s_test.csv'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/annakhazan/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \"\"\"\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/annakhazan/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[0;32m--> 317\u001b[0;31m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[0;31mValueError\u001b[0m: X has 55 features per sample; expecting 51"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Determine feature importance for each model and transformation combination\n",
    "with open('model_testing.txt', 'w+') as file:\n",
    "    for trans, s, X, X_val, cols in X_all:\n",
    "        for name,model,params in features:\n",
    "            for c in column_lists:\n",
    "                print (name)\n",
    "                file.write('name : ' + str(name) + '\\n')\n",
    "                print (model)\n",
    "                file.write('model : ' + str(model) + '\\n')\n",
    "                print (c)\n",
    "                file.write('c : ' + str(cols) + '\\n')\n",
    "\n",
    "                selected_features = column_lists[c]\n",
    "                print (len(selected_features))\n",
    "                file.write('selected features : ' + str(selected_features) + '\\n')\n",
    "\n",
    "                cols_list = [] # List of names of columns selected\n",
    "                i_cols_list = [] # Indexes of columns selected\n",
    "                rank_list =[] # Ranking of all the columns\n",
    "                rem_list = [] # List of columns not selected\n",
    "                i_rem_list = [] # Indexes of columns not selected\n",
    "\n",
    "                for field in cols:\n",
    "                    if field in selected_features:\n",
    "                        cols_list.append(field)\n",
    "                        i_cols_list.append(list(cols).index(field))\n",
    "                    else:\n",
    "                        rem_list.append(field)\n",
    "                        i_rem_list.append(list(cols).index(field))\n",
    "\n",
    "                #Limit training and validation dataset to just relevant columns\n",
    "                X_new = np.delete(X, i_rem_list, axis=1)\n",
    "                X_val_new = np.delete(X_val, i_rem_list, axis=1) \n",
    "\n",
    "                #Fit the model on selected dataset\n",
    "                model.fit(X_new, y_train)\n",
    "\n",
    "                #Calculate model score against true class for each sample\n",
    "                print (model.score(X_val_new, y_val))\n",
    "                file.write('model score : ' + str(model.score(X_val_new, y_val)) + '\\n')\n",
    "#                 #Grid search\n",
    "#                 file.write('Grid Search Results -- \\n')\n",
    "#                 best_estimator, best_score, best_params, cv_results = gridSearch(model, params, X_new, y_train)\n",
    "#                 print (best_estimator)\n",
    "#                 file.write('best estimator : ' + str(best_estimator) + '\\n')\n",
    "#                 print (best_score)\n",
    "#                 file.write('best score : ' + str(best_score) + '\\n')\n",
    "#                 print (best_params)\n",
    "#                 file.write('best params : ' + str(best_params) + '\\n')\n",
    "#                 print (cv_results)\n",
    "#                 file.write('best cv results : ' + str(cv_results) + '\\n')\n",
    "                \n",
    "                #Output prediction on test data\n",
    "                pd.DataFrame(model.predict(X_test)).to_csv('%s_%s_%s_test.csv'%(trans, name, c))\n",
    "                \n",
    "                print (confusion_matrix_scoring(model.predict(X_val_new), y_val))\n",
    "                file.write('conf matrix score : ' + str(confusion_matrix_scoring(model.predict(X_val_new), y_val)) + '\\n')\n",
    "                file.write('\\n')\n",
    "                file.write('-----------------\\n')\n",
    "                file.write('\\n')\n",
    "\n",
    "\n",
    "                # Append model name, array, columns selected and columns to be removed to the additional list        \n",
    "                X_all_add.append([trans,name,X_new,X_val_new,cols_list]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
