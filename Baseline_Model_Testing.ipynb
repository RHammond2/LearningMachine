{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Baseline Model Testing\n",
    "\n",
    "Data source: https://www.kaggle.com/c/forest-cover-type-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annakhazan/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import cross_validation\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import f_classif\n",
    "from IPython.core.display import display, HTML\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from feature_eng_function import feature_eng_forest, forest_interactions\n",
    "from confusion_matrix_score_function import confusion_matrix_scoring \n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest = pd.read_csv(\"data/train.csv\") \n",
    "forest = forest.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "original_cols = list(forest.columns)\n",
    "original_cols.remove('Cover_Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(original_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped the following columns: \n",
      "\n",
      "Wetmore\n",
      "Pachic Argiborolis\n",
      "Aquolis\n"
     ]
    }
   ],
   "source": [
    "forest = feature_eng_forest('data/train.csv', 'soil_types.csv')\n",
    "forest = forest.iloc[:,1:]\n",
    "original_cols_with_soil_eng = list(forest.columns)\n",
    "original_cols_with_soil_eng.remove('Cover_Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest = forest_interactions(forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the continuous features\n",
    "###### We will try Normalization, Standardized Scaling, and MinMax Scaling\n",
    "###### Note: there is no need to impute any data points as this is a pretty clean data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chunk_size = 0.1 #Validation chunk size\n",
    "seed = 0 # Use the same random seed to ensure consistent validation chunk usage\n",
    "\n",
    "X_all = [] # all features\n",
    "X_all_add = [] # Additionally we will make a list of subsets\n",
    "trans_list = [] # Transformations\n",
    "comb = [] # combinations\n",
    "comb.append(\"All+1.0\")\n",
    "\n",
    "features = [] # feature selection models\n",
    "model_features = [] # names of feature selection models\n",
    "\n",
    "#Reorder the data to have continuous variables come first\n",
    "continuous = []\n",
    "categorical = []\n",
    "final_columns = []\n",
    "for col in forest.columns.tolist():\n",
    "    if col == 'Cover_Type':\n",
    "        pass\n",
    "    elif forest[col].nunique() > 4:\n",
    "        continuous.append(col)\n",
    "    else:\n",
    "        categorical.append(col)\n",
    "final_columns.extend(continuous)\n",
    "final_columns.extend(categorical)\n",
    "final_columns.append('Cover_Type')\n",
    "forest = forest[final_columns]\n",
    "num_row, num_cols = forest.shape\n",
    "cols = forest.columns\n",
    "size = len(continuous) # Number of continuous columns\n",
    "\n",
    "#Create the data arrays for model building\n",
    "val_array = forest.values\n",
    "X = val_array[:,0:(num_cols-1)]\n",
    "y = val_array[:,(num_cols-1)]\n",
    "X_train, X_val, y_train, y_val = cross_validation.train_test_split(X, y, test_size=chunk_size, random_state=seed)\n",
    "X_all.append(['Orig','All', X_train,X_val,cols[:num_cols-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Standardize the data\n",
    "\n",
    "# X_temp = StandardScaler().fit_transform(X_train[:,0:size])\n",
    "# X_val_temp = StandardScaler().fit_transform(X_val[:,0:size])\n",
    "\n",
    "# # Recombine data\n",
    "# X_con = np.concatenate((X_temp,X_train[:,size:]),axis=1)\n",
    "# X_val_con = np.concatenate((X_val_temp,X_val[:,size:]),axis=1)\n",
    "\n",
    "# X_all.append(['StdSca','All', X_con,X_val_con,cols,rem,i_cols,i_rem])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MinMax Scale the data\n",
    "\n",
    "X_temp = MinMaxScaler().fit_transform(X_train[:,0:size])\n",
    "X_val_temp = MinMaxScaler().fit_transform(X_val[:,0:size])\n",
    "\n",
    "# Recombine data\n",
    "X_con = np.concatenate((X_temp,X_train[:,size:]),axis=1)\n",
    "X_val_con = np.concatenate((X_val_temp,X_val[:,size:]),axis=1)\n",
    "\n",
    "X_all.append(['MinMax', 'All', X_con,X_val_con,cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Normalize the data\n",
    "\n",
    "# X_temp = Normalizer().fit_transform(X_train[:,0:size])\n",
    "# X_val_temp = Normalizer().fit_transform(X_val[:,0:size])\n",
    "\n",
    "# # Recombine data\n",
    "# X_con = np.concatenate((X_temp,X_train[:,size:]),axis=1)\n",
    "# X_val_con = np.concatenate((X_val_temp,X_val[:,size:]),axis=1)\n",
    "\n",
    "# X_all.append(['Norm', 'All', X_con,X_val_con,cols,rem,i_cols,i_rem])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add transformation to the list\n",
    "for trans,name,X,X_val,cols_list in X_all:\n",
    "    trans_list.append(trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create classifiers and Grid Search\n",
    "- Logistic Regression\n",
    "- SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add Logistic Regression\n",
    "n = 'Logistic Regression'\n",
    "model_features.append(n)\n",
    "comb.append(n)\n",
    "features.append([n, LogisticRegression(random_state=seed),\n",
    "    {\n",
    "        'penalty':('l1', 'l2'),\n",
    "        'dual':(True, False),\n",
    "        'C':(1e-3, 1e-2,1e-1,1e0,1e1,1e2,1e3),\n",
    "        'fit_intercept':(True, False),\n",
    "        'intercept_scaling':(1e-3,1e-2,1e-1,1e0,1e1,1e2,1e3),\n",
    "        'max_iter':('newton-cg', 'lbfgs', 'liblinear', 'sag'),\n",
    "        'tol':(1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1),\n",
    "        'multi_class':('ovr', 'multinomial')\n",
    "    }])\n",
    "    \n",
    "# Add SVM\n",
    "n = 'SVM'\n",
    "model_features.append(n)\n",
    "comb.append(n)\n",
    "features.append([n, LinearSVC(random_state=seed),\n",
    "    {\n",
    "        'C':(1e-3,1e-2,1e-1,1e0,1e1,1e2,1e3),\n",
    "        'kernel':('linear', 'poly', 'rbf', 'sigmoid', 'precomputed'),\n",
    "        'degree':(1,2,3,4),\n",
    "        'gamma':('auto',1e-3, 1e-2,1e-1,1e0,1e1,1e2,1e3),\n",
    "        'coef0':(1e-3, 1e-2,1e-1,1e0,1e1,1e2,1e3),\n",
    "        'probability':(True,False),\n",
    "        'shrinking':(True,False),\n",
    "        'tol':(1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1),\n",
    "        'decision_function_shape':('ovo', 'ovr', None)\n",
    "    }])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 100 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top100 = list(pd.read_csv('top_100.csv', header=None, names=['Feature', 'Importance'])['Feature'].values)\n",
    "column_lists = {\n",
    "    'original':original_cols,\n",
    "    'original with soil engineered':original_cols_with_soil_eng,\n",
    "    'top 100':top100#,\n",
    "    #'all':all_interacted_cols\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run grid search over the different data transformations\n",
    "def gridSearch(model, params, X, y):\n",
    "    g = GridSearchCV(model, params, error_score=-999, verbose=1)\n",
    "    g.fit(X, y)\n",
    "    return g.best_estimator_, g.best_score_, g.best_params_, g.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run models on selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "original\n",
      "54\n",
      "0.630952380952\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_input_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-1b217c82e2eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m#                 file.write('best cv results : ' + str(cv_results) + '\\n')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                 \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m                 \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conf matrix score : '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/annakhazan/Desktop/LearningMachine/confusion_matrix_score_function.py\u001b[0m in \u001b[0;36mconfusion_matrix_scoring\u001b[0;34m(predicted_classes, true_classes)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \t[0, 0, 0, 0, 0, 0, 0]])\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_input_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                 \u001b[0mtrue_conf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_input_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_input_y2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mscore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_input_x' is not defined"
     ]
    }
   ],
   "source": [
    "# Determine feature importance for each model and transformation combination\n",
    "with open('model_testing.txt', 'w+') as file:\n",
    "    output = []\n",
    "    for trans, s, X, X_val, cols in X_all:\n",
    "        for name,model,params in features:\n",
    "            for c in column_lists:\n",
    "                print (name)\n",
    "                file.write('name : ' + str(name) + '\\n')\n",
    "                print (model)\n",
    "                file.write('model : ' + str(model) + '\\n')\n",
    "                print (c)\n",
    "                file.write('c : ' + str(cols) + '\\n')\n",
    "\n",
    "                selected_features = column_lists[c]\n",
    "                print (len(selected_features))\n",
    "                file.write('selected features : ' + str(selected_features) + '\\n')\n",
    "\n",
    "                cols_list = [] # List of names of columns selected\n",
    "                i_cols_list = [] # Indexes of columns selected\n",
    "                rank_list =[] # Ranking of all the columns\n",
    "                rem_list = [] # List of columns not selected\n",
    "                i_rem_list = [] # Indexes of columns not selected\n",
    "\n",
    "                for field in cols:\n",
    "                    if field in selected_features:\n",
    "                        cols_list.append(field)\n",
    "                        i_cols_list.append(list(cols).index(field))\n",
    "                    else:\n",
    "                        rem_list.append(field)\n",
    "                        i_rem_list.append(list(cols).index(field))\n",
    "\n",
    "                #Limit training and validation dataset to just relevant columns\n",
    "                X_new = np.delete(X, i_rem_list, axis=1)\n",
    "                X_val_new = np.delete(X_val, i_rem_list, axis=1) \n",
    "\n",
    "                #Fit the model on selected dataset\n",
    "                model.fit(X_new, y_train)\n",
    "\n",
    "                #Calculate model score against true class for each sample\n",
    "                print (model.score(X_val_new, y_val))\n",
    "                file.write('model score : ' + str(model.score(X_val_new, y_val)) + '\\n')\n",
    "#                 #Grid search\n",
    "#                 file.write('Grid Search Results -- \\n')\n",
    "#                 best_estimator, best_score, best_params, cv_results = gridSearch(model, params, X_new, y_train)\n",
    "#                 print (best_estimator)\n",
    "#                 file.write('best estimator : ' + str(best_estimator) + '\\n')\n",
    "#                 print (best_score)\n",
    "#                 file.write('best score : ' + str(best_score) + '\\n')\n",
    "#                 print (best_params)\n",
    "#                 file.write('best params : ' + str(best_params) + '\\n')\n",
    "#                 print (cv_results)\n",
    "#                 file.write('best cv results : ' + str(cv_results) + '\\n')\n",
    "\n",
    "                print (confusion_matrix_scoring(model.predict(X_val_new), y_val))\n",
    "                file.write('conf matrix score : ' + str(confusion_matrix_scoring(model.predict(X_val_new), y_val)) + '\\n')\n",
    "                file.write('\\n')\n",
    "                file.write('-----------------\\n')\n",
    "                file.write('\\n')\n",
    "\n",
    "\n",
    "                # Append model name, array, columns selected and columns to be removed to the additional list        \n",
    "                X_all_add.append([trans,name,X_new,X_val_new,cols_list]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
